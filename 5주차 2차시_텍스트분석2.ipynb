{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97652544",
   "metadata": {},
   "source": [
    "### 05. 감성 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14df70c9",
   "metadata": {},
   "source": [
    "#### 감성 분석 소개\n",
    "감성 분석(Sentiment Analysis)은 문서의 주관적인 감성/의견/감정/기분 등을 파악하기 위한 방법으로 소셜 미디어, 여론조사, 온라인 리뷰, 피드백 등 다양한 분야에서 활용되고 있음. 문서 내 텍스트가 나타내는 여러 가지 주관적인 단어와 문맥을 기반으로 감성(Sentiment) 수치를 계산하는 방법을 이용하는데, 긍정 감성 지수와 부정 감성 지수를 합산해 긍정 감성 또는 부정 감성을 결정함.\n",
    "- 지도학습은 학습 데이터와 타깃 레이블 값을 기반으로 감성 분석 학습을 수행한 뒤 이를 기반으로 다른 데이터의 감성 분석을 예측하는 방법.\n",
    "- 비지도학습은 'Lexicon'이라는 일종의 감성 어휘 사전을 이용함. Lexicon은 감성 분석을 위한 용어와 문맥에 대한 다양한 정보를 가지고 있으며, 이를 이용해 문서의 긍정적, 부정적 감성 여부를 판단함.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad3c2e7",
   "metadata": {},
   "source": [
    "#### 지도학습 기반 감성 분석 실습 - IMDB 영화평\n",
    "( cf. 감성 분석이지만 사실상 텍스트 기반의 이진 분류라고 볼 수 있음. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a681cffc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"5814_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"With all this stuff going down at the moment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"2381_9\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"7759_3\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"The film starts with a manager (Nicholas Bell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"3630_4\"</td>\n",
       "      <td>0</td>\n",
       "      <td>\"It must be assumed that those who praised thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"9495_8\"</td>\n",
       "      <td>1</td>\n",
       "      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  sentiment                                             review\n",
       "0  \"5814_8\"          1  \"With all this stuff going down at the moment ...\n",
       "1  \"2381_9\"          1  \"\\\"The Classic War of the Worlds\\\" by Timothy ...\n",
       "2  \"7759_3\"          0  \"The film starts with a manager (Nicholas Bell...\n",
       "3  \"3630_4\"          0  \"It must be assumed that those who praised thi...\n",
       "4  \"9495_8\"          1  \"Superbly trashy and wondrously unpretentious ..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "review_df = pd.read_csv('labeledTrainData.tsv', header=0, sep=\"\\t\", quoting=3)\n",
    "review_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6fa5196e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"With all this stuff going down at the moment with MJ i've started listening to his music, watching the odd documentary here and there, watched The Wiz and watched Moonwalker again. Maybe i just want to get a certain insight into this guy who i thought was really cool in the eighties just to maybe make up my mind whether he is guilty or innocent. Moonwalker is part biography, part feature film which i remember going to see at the cinema when it was originally released. Some of it has subtle messages about MJ's feeling towards the press and also the obvious message of drugs are bad m'kay.<br /><br />Visually impressive but of course this is all about Michael Jackson so unless you remotely like MJ in anyway then you are going to hate this and find it boring. Some may call MJ an egotist for consenting to the making of this movie BUT MJ and most of his fans would say that he made it for the fans which if true is really nice of him.<br /><br />The actual feature film bit when it finally starts is only on for 20 minutes or so excluding the Smooth Criminal sequence and Joe Pesci is convincing as a psychopathic all powerful drug lord. Why he wants MJ dead so bad is beyond me. Because MJ overheard his plans? Nah, Joe Pesci's character ranted that he wanted people to know it is he who is supplying drugs etc so i dunno, maybe he just hates MJ's music.<br /><br />Lots of cool things in this like MJ turning into a car and a robot and the whole Speed Demon sequence. Also, the director must have had the patience of a saint when it came to filming the kiddy Bad sequence as usually directors hate working with one kid let alone a whole bunch of them performing a complex dance scene.<br /><br />Bottom line, this movie is for people who like MJ on one level or another (which i think is most people). If not, then stay away. It does try and give off a wholesome message and ironically MJ's bestest buddy in this movie is a girl! Michael Jackson is truly one of the most talented people ever to grace this planet but is he guilty? Well, with all the attention i've gave this subject....hmmm well i don't know because people can be different behind closed doors, i know this for a fact. He is either an extremely nice but stupid guy or one of the most sickest liars. I hope he is not the latter.\"\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 구성 확인\n",
    "print(review_df['review'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a0d47af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# <br> html 태그는 replace 함수로 공백으로 변환\n",
    "review_df['review'] = review_df['review'].str.replace('<br />',' ')\n",
    "\n",
    "# 파이썬의 정규 표현식 모듈인 re를 이용하여 영어 문자열이 아닌 문자는 모두 공백으로 변환 \n",
    "review_df['review'] = review_df['review'].apply( lambda x : re.sub(\"[^a-zA-Z]\", \" \", x) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66a6ce8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17500, 1), (7500, 1))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class_df = review_df['sentiment'] # 결정값\n",
    "feature_df = review_df.drop(['id','sentiment'], axis=1, inplace=False) # 피처 데이터 세트\n",
    "\n",
    "X_train, X_test, y_train, y_test= train_test_split(feature_df, class_df, test_size=0.3, random_state=156)\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "63c95002",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8860, ROC-AUC는 0.9503\n"
     ]
    }
   ],
   "source": [
    "# 앞서 배운 Pipeline 객체를 이용해 피처 벡터화 및 알고리즘 적용\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# 스톱 워드는 English, filtering, ngram은 (1,2)로 설정해 CountVectorization수행. \n",
    "# LogisticRegression의 C는 10으로 설정. \n",
    "pipeline = Pipeline([\n",
    "    ('cnt_vect', CountVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "# Pipeline 객체를 이용하여 fit(), predict()로 학습/예측 수행. predict_proba()는 roc_auc때문에 수행.  \n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test ,pred),\n",
    "                                         roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36fc7065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 정확도는 0.8936, ROC-AUC는 0.9598\n"
     ]
    }
   ],
   "source": [
    "# 스톱 워드는 english, filtering, ngram은 (1,2)로 설정해 TF-IDF 벡터화 수행. \n",
    "# LogisticRegression의 C는 10으로 설정. \n",
    "pipeline = Pipeline([\n",
    "    ('tfidf_vect', TfidfVectorizer(stop_words='english', ngram_range=(1,2) )),\n",
    "    ('lr_clf', LogisticRegression(C=10))])\n",
    "\n",
    "pipeline.fit(X_train['review'], y_train)\n",
    "pred = pipeline.predict(X_test['review'])\n",
    "pred_probs = pipeline.predict_proba(X_test['review'])[:,1]\n",
    "\n",
    "print('예측 정확도는 {0:.4f}, ROC-AUC는 {1:.4f}'.format(accuracy_score(y_test ,pred),\n",
    "                                         roc_auc_score(y_test, pred_probs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52440c85",
   "metadata": {},
   "source": [
    "#### 비지도학습 기반 감성 분석 소개\n",
    "일반적으로 감성 분석용 데이터는 결정된 레이블 값을 가지고 있지 않은 경우가 많은데, 이 경우 Lexicon이 유용하게 사용될 수 있음. Lexicon은 감성 사전으로, 긍정 감성 또는 부정 감성의 정도를 의미하는 수치를 가지고 있으며 이를 감성 지수(Polarity score)라고 함. 감성 지수는 단어위 위치, 주변 단어, 문맥, POS(Part of Speech) 등을 참고해 결정되며, 이러한 감성 사전을 구현한 대표격은 NLTK 패키지임.  \n",
    "</br>\n",
    "NLP에서 제공하는 WordNet 모듈은 방대한 영어 어휘 사전으로, 시맨틱 분석을 제공함. 이때 시맨틱(semantic)이란, '문맥상 의미'를 뜻함. WordNet은 다양한 상황에서 같은 어휘라도 다르게 사용되는 어휘의 시맨틱 정보를 제공하며, 각각의 품사로 구성된 개별 단어를 단어가 가지는 시맨틱 정보를 제공하는 Synset(Sets of cognitive synonyms)을 이용해 표현함.  \n",
    "</br>\n",
    "NLTK 패키지는 훌륭한 감성 사전이지만, 예측 성능이 그리 좋지 않기 때문에 실제 업무의 적용은 아래의 감성 사전들을 적용하는 것이 일반적임.\n",
    "- SentiWordNet : 감성 단어 전용의 WordNet을 구현한 것으로, Synset 개념을 감성 분석에 적용한 것. 긍정 감성 지수, 부정 감성 지수, 객관성 지수를 할당하여 최종 감성 지수를 계산하고 이에 기반하여 감성이 긍정인지 부정인지 결정함. (이떄 객관성 지수는 단어가 감성과 관계없이 얼마나 객관적인지를 수치로 나타낸 것.)  \n",
    "( cf. SentiWordNet 설명 참고 : https://bab2min.tistory.com/573 )\n",
    "- VADER : 주로 소셜 미디어의 텍스트 감성 분석 제공을 위한 패키지. 뛰어난 감성 분석 결과를 제공하며, 비교적 빠른 수행 시간을 보장해 대용량 텍스트 데이터에 잘 사용됨.\n",
    "- Pattern : 예측 성능 측면에서 가장 주목받는 패키지이지만, 파이썬 3.X 버전에서 호환되지 않고 2.X 버전에서만 작동함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcc751",
   "metadata": {},
   "source": [
    "#### SentiWordNet을 이용한 감성 분석\n",
    "##### WordNet Synset과 SentiWordNet SentiSynset 클래스의 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad232a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b85704e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "synsets() 반환 type : <class 'list'>\n",
      "synsets() 반환 값 갯수: 18\n",
      "synsets() 반환 값 : [Synset('present.n.01'), Synset('present.n.02'), Synset('present.n.03'), Synset('show.v.01'), Synset('present.v.02'), Synset('stage.v.01'), Synset('present.v.04'), Synset('present.v.05'), Synset('award.v.01'), Synset('give.v.08'), Synset('deliver.v.01'), Synset('introduce.v.01'), Synset('portray.v.04'), Synset('confront.v.03'), Synset('present.v.12'), Synset('salute.v.06'), Synset('present.a.01'), Synset('present.a.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "term = 'present'\n",
    "\n",
    "# 'present'라는 단어로 wordnet의 synsets 생성. \n",
    "synsets = wn.synsets(term)\n",
    "print('synsets() 반환 type :', type(synsets))\n",
    "print('synsets() 반환 값 갯수:', len(synsets))\n",
    "print('synsets() 반환 값 :', synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b4d2ac",
   "metadata": {},
   "source": [
    "→ Synset 객체의 파라미터 'present.n.01'은 POS 태그를 나타냄. present는 의미, n은 품사, 01은 present가 명사로서 가지는 의미를 구분하는 인덱스."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4a1b7a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Synset name :  present.n.01 #####\n",
      "POS : noun.time\n",
      "Definition: the period of time that is happening now; any continuous stretch of time including the moment of speech\n",
      "Lemmas: ['present', 'nowadays']\n",
      "##### Synset name :  present.n.02 #####\n",
      "POS : noun.possession\n",
      "Definition: something presented as a gift\n",
      "Lemmas: ['present']\n",
      "##### Synset name :  present.n.03 #####\n",
      "POS : noun.communication\n",
      "Definition: a verb tense that expresses actions or states at the time of speaking\n",
      "Lemmas: ['present', 'present_tense']\n",
      "##### Synset name :  show.v.01 #####\n",
      "POS : verb.perception\n",
      "Definition: give an exhibition of to an interested audience\n",
      "Lemmas: ['show', 'demo', 'exhibit', 'present', 'demonstrate']\n",
      "##### Synset name :  present.v.02 #####\n",
      "POS : verb.communication\n",
      "Definition: bring forward and present to the mind\n",
      "Lemmas: ['present', 'represent', 'lay_out']\n",
      "##### Synset name :  stage.v.01 #####\n",
      "POS : verb.creation\n",
      "Definition: perform (a play), especially on a stage\n",
      "Lemmas: ['stage', 'present', 'represent']\n",
      "##### Synset name :  present.v.04 #####\n",
      "POS : verb.possession\n",
      "Definition: hand over formally\n",
      "Lemmas: ['present', 'submit']\n",
      "##### Synset name :  present.v.05 #####\n",
      "POS : verb.stative\n",
      "Definition: introduce\n",
      "Lemmas: ['present', 'pose']\n",
      "##### Synset name :  award.v.01 #####\n",
      "POS : verb.possession\n",
      "Definition: give, especially as an honor or reward\n",
      "Lemmas: ['award', 'present']\n",
      "##### Synset name :  give.v.08 #####\n",
      "POS : verb.possession\n",
      "Definition: give as a present; make a gift of\n",
      "Lemmas: ['give', 'gift', 'present']\n",
      "##### Synset name :  deliver.v.01 #####\n",
      "POS : verb.communication\n",
      "Definition: deliver (a speech, oration, or idea)\n",
      "Lemmas: ['deliver', 'present']\n",
      "##### Synset name :  introduce.v.01 #####\n",
      "POS : verb.communication\n",
      "Definition: cause to come to know personally\n",
      "Lemmas: ['introduce', 'present', 'acquaint']\n",
      "##### Synset name :  portray.v.04 #####\n",
      "POS : verb.creation\n",
      "Definition: represent abstractly, for example in a painting, drawing, or sculpture\n",
      "Lemmas: ['portray', 'present']\n",
      "##### Synset name :  confront.v.03 #####\n",
      "POS : verb.communication\n",
      "Definition: present somebody with something, usually to accuse or criticize\n",
      "Lemmas: ['confront', 'face', 'present']\n",
      "##### Synset name :  present.v.12 #####\n",
      "POS : verb.communication\n",
      "Definition: formally present a debutante, a representative of a country, etc.\n",
      "Lemmas: ['present']\n",
      "##### Synset name :  salute.v.06 #####\n",
      "POS : verb.communication\n",
      "Definition: recognize with a gesture prescribed by a military regulation; assume a prescribed position\n",
      "Lemmas: ['salute', 'present']\n",
      "##### Synset name :  present.a.01 #####\n",
      "POS : adj.all\n",
      "Definition: temporal sense; intermediate between past and future; now existing or happening or in consideration\n",
      "Lemmas: ['present']\n",
      "##### Synset name :  present.a.02 #####\n",
      "POS : adj.all\n",
      "Definition: being or existing in a specified place\n",
      "Lemmas: ['present']\n"
     ]
    }
   ],
   "source": [
    "for synset in synsets :\n",
    "    print('##### Synset name : ', synset.name(),'#####')\n",
    "    print('POS :',synset.lexname())\n",
    "    print('Definition:',synset.definition())\n",
    "    print('Lemmas:',synset.lemma_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848f240",
   "metadata": {},
   "source": [
    "→ 이처럼 **<font color=orange>synset은 하나의 단어가 가질 수 있는 여러 가지 시맨틱 정보를 개별 클래스로 나타낸 것.</font>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94cef402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tree</th>\n",
       "      <th>lion</th>\n",
       "      <th>tiger</th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>tree</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>0.07</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tiger</th>\n",
       "      <td>0.07</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.08</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.12</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tree  lion  tiger   cat   dog\n",
       "tree   1.00  0.07   0.07  0.08  0.12\n",
       "lion   0.07  1.00   0.33  0.25  0.17\n",
       "tiger  0.07  0.33   1.00  0.25  0.17\n",
       "cat    0.08  0.25   0.25  1.00  0.20\n",
       "dog    0.12  0.17   0.17  0.20  1.00"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WordNet은 단어의 유사도 표현 가능\n",
    "\n",
    "# synset 객체를 단어별로 생성합니다. \n",
    "tree = wn.synset('tree.n.01')\n",
    "lion = wn.synset('lion.n.01')\n",
    "tiger = wn.synset('tiger.n.02')\n",
    "cat = wn.synset('cat.n.01')\n",
    "dog = wn.synset('dog.n.01')\n",
    "\n",
    "entities = [tree , lion , tiger , cat , dog]\n",
    "similarities = []\n",
    "entity_names = [ entity.name().split('.')[0] for entity in entities]\n",
    "\n",
    "# 단어별 synset 들을 iteration 하면서 다른 단어들의 synset과 유사도를 측정합니다. \n",
    "for entity in entities:\n",
    "    similarity = [ round(entity.path_similarity(compared_entity), 2)  for compared_entity in entities ]\n",
    "    similarities.append(similarity)\n",
    "    \n",
    "# 개별 단어별 synset과 다른 단어의 synset과의 유사도를 DataFrame형태로 저장합니다.  \n",
    "similarity_df = pd.DataFrame(similarities , columns=entity_names,index=entity_names)\n",
    "similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02542e8f",
   "metadata": {},
   "source": [
    "SentiWordNet은 WordNet의 Synset과 유사한 Senti_Synset 클래스를 가지고 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0aa3e27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti_synsets() 반환 type : <class 'list'>\n",
      "senti_synsets() 반환 값 갯수: 11\n",
      "senti_synsets() 반환 값 : [SentiSynset('decelerate.v.01'), SentiSynset('slow.v.02'), SentiSynset('slow.v.03'), SentiSynset('slow.a.01'), SentiSynset('slow.a.02'), SentiSynset('dense.s.04'), SentiSynset('slow.a.04'), SentiSynset('boring.s.01'), SentiSynset('dull.s.08'), SentiSynset('slowly.r.01'), SentiSynset('behind.r.03')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "senti_synsets = list(swn.senti_synsets('slow'))\n",
    "print('senti_synsets() 반환 type :', type(senti_synsets))\n",
    "print('senti_synsets() 반환 값 갯수:', len(senti_synsets))\n",
    "print('senti_synsets() 반환 값 :', senti_synsets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d304ae",
   "metadata": {},
   "source": [
    "SentiSynset 객체는 단어의 감성을 나타내는 **<font color=orange>감성 지수</font>**와, 감성과 반대되는 **<font color=orange>객관성 지수</font>**를 가지고 있으며, 감성 지수는 **긍정 감성 지수**와 **부정 감성 지수**로 나뉨. 만약 어떤 단어가 전혀 감성적이지 않다면, 감성 지수는 모두 0이 되며 객관성 지수는 1이 됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0e7c57db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father 긍정감성 지수:  0.0\n",
      "father 부정감성 지수:  0.0\n",
      "father 객관성 지수:  1.0\n",
      "\n",
      "\n",
      "fabulous 긍정감성 지수:  0.875\n",
      "fabulous 부정감성 지수:  0.125\n"
     ]
    }
   ],
   "source": [
    "# father, fabulous 감성지수와 객관성지수 확인\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "father = swn.senti_synset('father.n.01')\n",
    "print('father 긍정감성 지수: ', father.pos_score())\n",
    "print('father 부정감성 지수: ', father.neg_score())\n",
    "print('father 객관성 지수: ', father.obj_score())\n",
    "print('\\n')\n",
    "fabulous = swn.senti_synset('fabulous.a.01')\n",
    "print('fabulous 긍정감성 지수: ',fabulous .pos_score())\n",
    "print('fabulous 부정감성 지수: ',fabulous .neg_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b253281",
   "metadata": {},
   "source": [
    "##### SentiWordNet을 이용한 영화 감상평 감성 분석\n",
    "<SentiWordNet 감성 분석 수행 개략적 순서>\n",
    "1. 문서(Document)를 문장(Sentence) 단위로 분해\n",
    "2. 다시 문장을 단어(Word) 단위로 토큰화하고 품사 태깅\n",
    "3. 품사 태깅된 단어 기반으로 synset 객체와 senti_synset 객체를 생성\n",
    "4. Senti_synset에서 긍정 감성/부정 감성 지수를 구하고 이를 모두 합산해 특정 임계치 값 이상일 떄 긍정 감성으로, 그렇지 않을 떄는 부정 감성으로 결정  \n",
    "\n",
    "SentiWordNet을 이용하기 위해서 WordNet을 이용하여 문서를 단어로 토큰화한 뒤, 어근 추출(Lemmatization)과 품사 태깅(POS Tagging)을 적용해야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48a68613",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 함수 생성\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# 간단한 NTLK PennTreebank Tag를 기반으로 WordNet기반의 품사 Tag로 변환\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d15c16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "\n",
    "def swn_polarity(text):\n",
    "    # 감성 지수 초기화 \n",
    "    sentiment = 0.0\n",
    "    tokens_count = 0\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    raw_sentences = sent_tokenize(text)\n",
    "    # 분해된 문장별로 단어 토큰 -> 품사 태깅 후에 SentiSynset 생성 -> 감성 지수 합산 \n",
    "    for raw_sentence in raw_sentences:\n",
    "        # NTLK 기반의 품사 태깅 문장 추출  \n",
    "        tagged_sentence = pos_tag(word_tokenize(raw_sentence))\n",
    "        for word , tag in tagged_sentence:\n",
    "            \n",
    "            # WordNet 기반 품사 태깅과 어근 추출\n",
    "            wn_tag = penn_to_wn(tag)\n",
    "            if wn_tag not in (wn.NOUN , wn.ADJ, wn.ADV):\n",
    "                continue                   \n",
    "            lemma = lemmatizer.lemmatize(word, pos=wn_tag)\n",
    "            if not lemma:\n",
    "                continue\n",
    "            # 어근을 추출한 단어와 WordNet 기반 품사 태깅을 입력해 Synset 객체를 생성. \n",
    "            synsets = wn.synsets(lemma , pos=wn_tag)\n",
    "            if not synsets:\n",
    "                continue\n",
    "            # sentiwordnet의 감성 단어 분석으로 감성 synset 추출\n",
    "            # 모든 단어에 대해 긍정 감성 지수는 +로 부정 감성 지수는 -로 합산해 감성 지수 계산. \n",
    "            synset = synsets[0]\n",
    "            swn_synset = swn.senti_synset(synset.name())\n",
    "            sentiment += (swn_synset.pos_score() - swn_synset.neg_score())           \n",
    "            tokens_count += 1\n",
    "    \n",
    "    if not tokens_count:\n",
    "        return 0\n",
    "    \n",
    "    # 총 score가 0 이상일 경우 긍정(Positive) 1, 그렇지 않을 경우 부정(Negative) 0 반환\n",
    "    if sentiment >= 0 :\n",
    "        return 1\n",
    "    \n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "927a6db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_df['preds'] = review_df['review'].apply( lambda x : swn_polarity(x) )\n",
    "y_target = review_df['sentiment'].values\n",
    "preds = review_df['preds'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0131e6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7668 4832]\n",
      " [3636 8864]]\n",
      "정확도: 0.6613\n",
      "정밀도: 0.6472\n",
      "재현율: 0.7091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score \n",
    "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "print(confusion_matrix(y_target, preds))\n",
    "print('정확도:', np.round(accuracy_score(y_target,preds),4))\n",
    "print('정밀도:', np.round(precision_score(y_target,preds),4))\n",
    "print('재현율:', np.round(recall_score(y_target,preds),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6f12d5",
   "metadata": {},
   "source": [
    "#### VADER를 이용한 감성분석\n",
    "VADER는 소셜 미디어의 감성 분석 용도로 만들어진 룰 기반의 Lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "da124802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.13, 'neu': 0.743, 'pos': 0.127, 'compound': -0.7943}\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "senti_analyzer = SentimentIntensityAnalyzer()\n",
    "senti_scores = senti_analyzer.polarity_scores(review_df['review'][0])\n",
    "print(senti_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1e033c",
   "metadata": {},
   "source": [
    "→ 'neg' : 부정 감성 지수, 'neu' : 중립적인 감성 지수, 'pos' : 긍정 감성 지수, 'compound' : 앞 세 가지 지수를 적절히 조합하여 -1~1 사이로 나타낸 값으로, 보통 0.1 이상이면 긍정 감성으로 판단하지만 상황에 따라 이 임계값을 적절히 조정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b948c45b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 6747  5753]\n",
      " [ 1858 10642]]\n",
      "정확도: 0.6956\n",
      "정밀도: 0.6491\n",
      "재현율: 0.8514\n"
     ]
    }
   ],
   "source": [
    "def vader_polarity(review,threshold=0.1):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    \n",
    "    # compound 값에 기반하여 threshold 입력값보다 크면 1, 그렇지 않으면 0을 반환 \n",
    "    agg_score = scores['compound']\n",
    "    final_sentiment = 1 if agg_score >= threshold else 0\n",
    "    return final_sentiment\n",
    "\n",
    "# apply lambda 식을 이용하여 레코드별로 vader_polarity( )를 수행하고 결과를 'vader_preds'에 저장\n",
    "review_df['vader_preds'] = review_df['review'].apply( lambda x : vader_polarity(x, 0.1) )\n",
    "y_target = review_df['sentiment'].values\n",
    "vader_preds = review_df['vader_preds'].values\n",
    "\n",
    "print(confusion_matrix(y_target, vader_preds))\n",
    "print('정확도:', np.round(accuracy_score(y_target,vader_preds),4))\n",
    "print('정밀도:', np.round(precision_score(y_target,vader_preds),4))\n",
    "print('재현율:', np.round(recall_score(y_target,vader_preds),4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d8727",
   "metadata": {},
   "source": [
    "→ 정확도가 SentiWordNet보다 향상됐으며, 재현율은 크게 향상됨.  \n",
    "물론 지도학습 분류 기반의 예측 성능에 비해 낮은 수준이지만, 결정 클래스 값이 없는 상황을 고려한다면 예측 성능에 일정 수준 만족할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e12952b",
   "metadata": {},
   "source": [
    "### 06. 토픽 모델링(Topic Modeling) - 20뉴스그룹"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98312e3c",
   "metadata": {},
   "source": [
    "**토픽 모델링**이란 문서 집합에 숨어 있는 주제를 찾아내는 것. 사람이 수행하는 토픽 모델링은 더 함축적인 의미로 문장을 요약하는 것에 반해, 머신러닝 기반의 토픽 모델링은 숨겨진 주제를 효과적으로 표현할 수 있는 중심 단어를 함축적으로 추출함.  \n",
    "토픽 모델링에 자주 사용되는 기법은 **<font color=orange>LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)</font>**. 본 장에서는 LDA를 이용해 토픽 모델링을 수행하고자 함.  \n",
    "\n",
    "(cf. LSA 설명 및 예제 : https://bkshin.tistory.com/entry/NLP-9-%EC%BD%94%EC%82%AC%EC%9D%B8-%EC%9C%A0%EC%82%AC%EB%8F%84%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-%EC%98%81%ED%99%94-%EC%B6%94%EC%B2%9C-%EC%8B%9C%EC%8A%A4%ED%85%9C )  \n",
    "(cf. LSA와 LDA 비교 : https://codong.tistory.com/36 )  \n",
    "(cf. 이때 LDA는 차원 축소의 LDA(Linear Discriminant Analysis)와 다른 알고리즘이므로 유의할 것!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "93d78c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer Shape: (7862, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# 모토사이클, 야구, 그래픽스, 윈도우즈, 중동, 기독교, 의학, 우주 주제를 추출. \n",
    "cats = ['rec.motorcycles', 'rec.sport.baseball', 'comp.graphics', 'comp.windows.x',\n",
    "        'talk.politics.mideast', 'soc.religion.christian', 'sci.electronics', 'sci.med'  ]\n",
    "\n",
    "# 위에서 cats 변수로 기재된 category만 추출. featch_20newsgroups( )의 categories에 cats 입력\n",
    "news_df= fetch_20newsgroups(subset='all',remove=('headers', 'footers', 'quotes'), \n",
    "                            categories=cats, random_state=0)\n",
    "\n",
    "#LDA 는 Count기반의 Vectorizer만 적용합니다.  \n",
    "count_vect = CountVectorizer(max_df=0.95, max_features=1000, min_df=2, stop_words='english', ngram_range=(1,2))\n",
    "feat_vect = count_vect.fit_transform(news_df.data)\n",
    "print('CountVectorizer Shape:', feat_vect.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80f98866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(n_components=8, random_state=0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LDA 토픽 모델링 수행\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=8, random_state=0) # 토픽의 개수 8개\n",
    "lda.fit(feat_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9601c59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 1000)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[3.60992018e+01, 1.35626798e+02, 2.15751867e+01, ...,\n",
       "        3.02911688e+01, 8.66830093e+01, 6.79285199e+01],\n",
       "       [1.25199920e-01, 1.44401815e+01, 1.25045596e-01, ...,\n",
       "        1.81506995e+02, 1.25097844e-01, 9.39593286e+01],\n",
       "       [3.34762663e+02, 1.25176265e-01, 1.46743299e+02, ...,\n",
       "        1.25105772e-01, 3.63689741e+01, 1.25025218e-01],\n",
       "       ...,\n",
       "       [3.60204965e+01, 2.08640688e+01, 4.29606813e+00, ...,\n",
       "        1.45056650e+01, 8.33854413e+00, 1.55690009e+01],\n",
       "       [1.25128711e-01, 1.25247756e-01, 1.25005143e-01, ...,\n",
       "        9.17278769e+01, 1.25177668e-01, 3.74575887e+01],\n",
       "       [5.49258690e+01, 4.47009532e+00, 9.88524814e+00, ...,\n",
       "        4.87048440e+01, 1.25034678e-01, 1.25074632e-01]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(lda.components_.shape)\n",
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c709360b",
   "metadata": {},
   "source": [
    "→ components_는 개별 토픽별로 각 word 피처가 얼마나 많이 할당됐는지에 대한 수치로, 높은 값일수록 토픽의 중심 word가 됨. 8개의 토픽별로 1000개의 word 피처가 해당 토픽별로 연관도 값을 가지고 있음을 확인."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b9df85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.60992018e+01, 1.35626798e+02, 2.15751867e+01, 1.66797374e+01,\n",
       "       2.79116453e+01, 5.10887470e+01, 2.71578118e+01, 5.63629854e+02,\n",
       "       1.51255012e+02, 2.50002658e+02, 3.43889076e+02, 9.41519034e+00,\n",
       "       1.24391406e+02, 1.48332625e+02, 2.41047913e+02, 1.15178467e+02,\n",
       "       1.67518832e+02, 1.24543573e+02, 9.39264574e+01, 8.98408568e+01,\n",
       "       1.15026060e+02, 1.66681056e+02, 3.18336067e+02, 3.40871322e+02,\n",
       "       6.57367239e+01, 9.55808129e+01, 9.42926924e+01, 8.20757241e+01,\n",
       "       9.75227950e+01, 1.25015126e-01, 2.07928452e+02, 8.49170785e+00,\n",
       "       6.82592012e+01, 8.92273844e+01, 1.06748151e+02, 7.28637466e+01,\n",
       "       2.00716963e+02, 9.39478917e+01, 9.61297746e+01, 5.65332001e+01,\n",
       "       1.08823005e+02, 1.25033129e-01, 7.85519321e+01, 3.97062801e+01,\n",
       "       1.58121306e+02, 9.82950167e+01, 6.04053607e+01, 7.02396275e+01,\n",
       "       4.26303416e+01, 9.26626954e+01, 5.07120905e+01, 2.66538978e+02,\n",
       "       8.84867374e+01, 5.50914847e+01, 2.98159374e+01, 1.35476116e-01,\n",
       "       3.47709761e+00, 2.68655460e-01, 1.27766191e+01, 3.72404379e+01,\n",
       "       6.68419408e+00, 1.26835900e+01, 7.94931485e+00, 1.25022634e-01,\n",
       "       5.57997688e+01, 3.03323536e+01, 1.85336430e+01, 4.61625699e+01,\n",
       "       1.14260434e+01, 1.25039697e-01, 6.99016886e+00, 1.08956572e+02,\n",
       "       4.79581661e+01, 1.58964448e+01, 2.08788569e+02, 1.56312248e+02,\n",
       "       7.78607100e+00, 8.13360897e+01, 1.25024620e-01, 5.18072591e+01,\n",
       "       1.25098293e-01, 1.25096568e-01, 7.69574229e+00, 8.30832321e+00,\n",
       "       3.29296914e+01, 6.64146692e+00, 1.25006005e-01, 7.07099920e+00,\n",
       "       7.82902116e+00, 2.06679308e+01, 1.26061811e-01, 5.24416779e+00,\n",
       "       1.73000705e+01, 7.92287081e-01, 6.23766640e-01, 1.70324647e+01,\n",
       "       1.64200604e+01, 2.36870554e+02, 1.25112541e-01, 1.25008807e-01,\n",
       "       1.25034919e-01, 3.06652470e+01, 3.14963534e+01, 6.81165481e+00,\n",
       "       1.25003844e-01, 2.59797496e+01, 1.25003023e-01, 1.25003171e-01,\n",
       "       1.25003072e-01, 1.25168014e-01, 2.04218896e+01, 3.64978305e+01,\n",
       "       1.93196787e+01, 4.26278492e+00, 3.85189682e+00, 2.20188264e+00,\n",
       "       5.10991952e+00, 1.65478582e+00, 3.32076437e+00, 5.94996558e+01,\n",
       "       2.24977840e+02, 1.28016827e+01, 2.15133236e+01, 1.25006949e-01,\n",
       "       1.25037313e-01, 7.42310327e+01, 1.30162906e+02, 1.08285860e+02,\n",
       "       2.66536157e+02, 6.96565793e+01, 2.35465793e+01, 3.82154510e+01,\n",
       "       1.25114891e-01, 1.45389853e+00, 6.43269018e+01, 1.32253846e+02,\n",
       "       2.27970023e+02, 1.25009609e-01, 5.26828754e+01, 1.25315427e-01,\n",
       "       1.48435113e+01, 1.25038305e-01, 1.71451674e+01, 9.89548599e+01,\n",
       "       2.44517191e+01, 1.98336930e+00, 3.32807176e+01, 1.59425101e+01,\n",
       "       1.25097001e-01, 1.55176632e+01, 3.65352676e+01, 1.38267958e+01,\n",
       "       2.55975529e+01, 1.25134937e-01, 2.79262800e+00, 1.25674278e+00,\n",
       "       1.95887193e+00, 5.14915972e+00, 1.25070480e-01, 1.09978572e+01,\n",
       "       8.21911357e+00, 5.77609225e+01, 3.63757105e+01, 1.53626137e+01,\n",
       "       3.19923940e+02, 1.25075528e-01, 1.64306972e-01, 1.14181603e+02,\n",
       "       5.81745265e+01, 8.66673611e+01, 1.25023923e-01, 7.60034837e+01,\n",
       "       1.51508301e+02, 4.71765764e+00, 1.98248712e+01, 4.32697351e+01,\n",
       "       4.15476896e+01, 3.12910334e+01, 1.59836219e+01, 4.53056094e+01,\n",
       "       1.25001372e-01, 1.55345327e+01, 6.72987019e+01, 9.46957912e+01,\n",
       "       1.25093212e-01, 6.71135757e+00, 1.25008717e-01, 1.25023714e-01,\n",
       "       1.25010652e-01, 1.25006342e-01, 1.25036524e-01, 1.25046924e-01,\n",
       "       1.60650773e+01, 1.42216115e+01, 1.82892439e+01, 1.37715535e+01,\n",
       "       1.93440416e+01, 1.74025451e+01, 1.25018170e-01, 1.25129837e-01,\n",
       "       1.49033258e+02, 3.26594678e+01, 1.25033473e-01, 1.25182417e-01,\n",
       "       1.25059444e-01, 2.99425015e+01, 2.45062084e+01, 2.17220621e+01,\n",
       "       1.80421396e+01, 1.25132454e-01, 1.72956239e+01, 1.39267927e+01,\n",
       "       6.07185554e+01, 4.51545722e+01, 1.25065550e-01, 1.56869665e+01,\n",
       "       2.56245428e+01, 1.78512944e+01, 1.29323475e+00, 1.17240344e+01,\n",
       "       2.17817559e+01, 3.12921229e+01, 3.11879196e+01, 1.90474769e+01,\n",
       "       4.92961997e+00, 1.61945941e+01, 1.25001875e-01, 9.94760055e+01,\n",
       "       1.25167757e-01, 2.24624340e-01, 7.01034468e+00, 2.91865505e+00,\n",
       "       4.87362750e+01, 2.97829669e+00, 1.81591991e+01, 1.38721086e+01,\n",
       "       3.82873735e+00, 1.93591472e+01, 5.31930693e+01, 9.58110617e-01,\n",
       "       1.30091289e+01, 3.63146098e+01, 1.99382281e+01, 3.36518907e+01,\n",
       "       2.31916693e+01, 2.04100088e+01, 7.34045285e+01, 3.96662368e+01,\n",
       "       4.36917533e+01, 1.76809941e+02, 5.74181297e+01, 4.30511063e+00,\n",
       "       9.87595906e+00, 2.71668204e+01, 1.49408998e-01, 1.25009173e-01,\n",
       "       5.26126931e+00, 9.68861244e+00, 1.16106781e+01, 6.72914927e+00,\n",
       "       5.46398479e+01, 1.52126772e+01, 4.30708459e+00, 5.75919458e+00,\n",
       "       1.97709340e+01, 2.04501075e+00, 4.68824968e+01, 7.11823372e+01,\n",
       "       6.98263002e+01, 1.08938858e+01, 1.85932576e+00, 6.18168331e+00,\n",
       "       1.58124605e+02, 2.69071518e+01, 3.51827258e+01, 8.94961824e+00,\n",
       "       1.25065090e-01, 1.76414244e+01, 1.25015841e-01, 1.72165151e+01,\n",
       "       3.32127594e+02, 1.25667954e-01, 1.25036821e-01, 5.23915992e+00,\n",
       "       5.21266256e+01, 1.25120828e-01, 6.74943218e+01, 1.25079897e-01,\n",
       "       6.04400995e+01, 1.86810789e+00, 1.45839084e+01, 9.57020852e+01,\n",
       "       3.23431456e+00, 1.10092197e+01, 3.40506969e+00, 1.25168803e-01,\n",
       "       1.25002296e-01, 1.10411856e+02, 6.84292026e+00, 1.50656498e+02,\n",
       "       3.63862700e+00, 9.87253872e+00, 1.25069771e-01, 1.77147771e+02,\n",
       "       3.01970932e+01, 1.33579892e+02, 4.82032603e+00, 8.99738164e+00,\n",
       "       5.93807586e+01, 1.04291599e+01, 1.25008953e-01, 2.00993906e+01,\n",
       "       1.05502107e+02, 8.57302591e+01, 1.16956447e+02, 1.25119881e-01,\n",
       "       9.49287809e+01, 1.66707251e+01, 1.20508808e+00, 1.46845912e+00,\n",
       "       6.38742688e+00, 1.90567771e+01, 3.17649599e+01, 1.05809742e+01,\n",
       "       1.71063268e+00, 3.66649581e+00, 3.54213947e+01, 6.63524516e+00,\n",
       "       2.18478350e+01, 7.69047344e-01, 1.96020068e+00, 4.91467721e+01,\n",
       "       2.19397976e+01, 1.25028014e-01, 9.03354118e+01, 6.18497809e+00,\n",
       "       2.83906969e+00, 3.19248087e+00, 6.08143443e+01, 1.34818593e+01,\n",
       "       1.25053260e-01, 1.25092468e-01, 2.92891075e+01, 1.25021187e-01,\n",
       "       3.26891894e+01, 1.81998992e+01, 1.16418199e+01, 1.25063774e-01,\n",
       "       1.12973147e+01, 1.27276579e-01, 8.71956054e+00, 1.07227067e+02,\n",
       "       1.04426748e+01, 1.25041244e-01, 1.25055284e-01, 3.14392828e+01,\n",
       "       1.93617160e+01, 6.57238159e+00, 1.87946097e+01, 5.59867429e+01,\n",
       "       1.25009712e-01, 1.25001325e-01, 1.84168381e+02, 3.93045860e+00,\n",
       "       4.86162305e+01, 1.25185898e-01, 1.25022146e-01, 1.25051291e-01,\n",
       "       3.51500275e+01, 4.16016396e+00, 1.25038767e-01, 4.81919018e+00,\n",
       "       2.86290934e-01, 5.52563167e+01, 4.76288060e+02, 3.17033063e+02,\n",
       "       1.35849818e+01, 4.51294783e+01, 1.10313545e+01, 1.25003927e-01,\n",
       "       4.88950107e+01, 5.52734650e+01, 1.25011538e-01, 7.98159758e+01,\n",
       "       3.40651097e+01, 2.12040181e+01, 1.25025238e-01, 1.15735631e+01,\n",
       "       3.18094615e+01, 6.05370816e+00, 2.86251697e+02, 4.08015137e+01,\n",
       "       1.25046059e-01, 2.17691083e+00, 1.25029549e-01, 4.54545181e+01,\n",
       "       9.15728913e+00, 1.25080956e-01, 1.25030064e-01, 1.27179731e-01,\n",
       "       7.25342522e+00, 2.40421835e+01, 2.27218024e+01, 1.73743277e+01,\n",
       "       1.44826679e+01, 1.96301977e+01, 3.35077330e+01, 1.71726229e+01,\n",
       "       3.37887836e+00, 5.19960942e-01, 4.30572674e+00, 3.73366978e+00,\n",
       "       1.66914241e+01, 2.19848230e+00, 3.32446414e+01, 1.25048558e-01,\n",
       "       1.36732870e+00, 1.05686190e+01, 4.23327703e+01, 9.47367511e+00,\n",
       "       3.77369043e+02, 3.88072103e+00, 1.91947873e+00, 6.28889507e+01,\n",
       "       1.25004657e-01, 4.81366314e+00, 9.21283344e-01, 8.50441571e+01,\n",
       "       1.25172213e-01, 1.72518319e+02, 6.96983379e+01, 3.81320550e+01,\n",
       "       2.27252525e+02, 2.46124985e+02, 1.51889576e+01, 1.25016257e-01,\n",
       "       7.53407180e+01, 2.17666825e+00, 1.25051015e-01, 2.63605965e+01,\n",
       "       5.31477915e+01, 1.21592151e+01, 2.74257630e+01, 3.25877755e+01,\n",
       "       1.41530026e-01, 2.09324899e+00, 7.90630407e+01, 1.46614901e+01,\n",
       "       2.04113103e+01, 5.57685735e-01, 1.47987764e+01, 1.25098597e-01,\n",
       "       1.25026122e-01, 6.14028042e-01, 7.38152469e+01, 3.64139335e+01,\n",
       "       1.32775480e+01, 2.71488079e+01, 3.90041001e+01, 2.42198258e+01,\n",
       "       1.55766151e+02, 3.59923629e+00, 1.25915681e+02, 2.18067048e-01,\n",
       "       7.69145381e-01, 2.48222150e+01, 6.01043406e+00, 1.78128639e+01,\n",
       "       1.71508229e+01, 1.25032690e-01, 2.52482144e+01, 3.10598246e+01,\n",
       "       2.24907771e+01, 2.24661897e+01, 1.25047187e-01, 1.25014340e-01,\n",
       "       1.94641034e+01, 2.99637599e+01, 1.25001449e-01, 1.25035435e-01,\n",
       "       1.25061542e-01, 1.25008891e-01, 2.56547718e+01, 6.49650419e+01,\n",
       "       1.25000490e-01, 5.98917989e+01, 1.36849248e+01, 1.25037519e-01,\n",
       "       3.13992857e+00, 1.25080934e-01, 6.63830500e+00, 1.99158798e+00,\n",
       "       2.93577206e+01, 1.61902180e+01, 3.85359615e+01, 4.13854799e+00,\n",
       "       2.91275582e-01, 1.25090862e-01, 1.25153494e-01, 5.10196758e+01,\n",
       "       3.34173642e+00, 1.90380965e+01, 2.48896665e+01, 1.25003812e-01,\n",
       "       1.25003804e-01, 7.05813012e+01, 1.87538994e+02, 1.99745183e+00,\n",
       "       1.42398880e+00, 2.05068675e+01, 4.16150975e+01, 3.13319278e+01,\n",
       "       8.79833213e+01, 1.25002700e-01, 1.82987418e-01, 2.55102216e+01,\n",
       "       1.59573388e+01, 9.05870526e+01, 5.18454792e+01, 9.13851544e+00,\n",
       "       3.13215122e+01, 8.12434485e+00, 1.44198224e+01, 5.54630375e+01,\n",
       "       1.83371131e+01, 1.25082065e-01, 5.01089105e+00, 1.40014321e+01,\n",
       "       3.95464588e+01, 3.26547713e+01, 1.17231218e+02, 2.99183945e+01,\n",
       "       4.20572381e+01, 1.34350282e+01, 3.00687597e+00, 1.80853794e+01,\n",
       "       2.04800077e+00, 4.06350256e+01, 7.39109697e+01, 2.74881463e+01,\n",
       "       6.33413909e+00, 1.23905405e+02, 1.25104635e-01, 4.89995446e-01,\n",
       "       1.79029756e+00, 3.32677456e+00, 1.25055006e-01, 7.47723088e+00,\n",
       "       8.17080299e+01, 1.01973267e+02, 3.39750423e+01, 2.79848807e+01,\n",
       "       1.60144531e+01, 3.12967035e+01, 5.07387862e+00, 4.84604113e+01,\n",
       "       1.25011055e-01, 1.20666411e+00, 1.54946903e+01, 2.53201991e+01,\n",
       "       2.78904096e+01, 1.25143891e-01, 1.41775073e+01, 4.13175174e+02,\n",
       "       1.07774440e+02, 1.24334517e+01, 6.67369386e+00, 5.43697080e+00,\n",
       "       2.77874399e+01, 1.47396769e+00, 1.42751096e+00, 1.09778753e+01,\n",
       "       1.13733352e+01, 7.03384843e+00, 5.71864834e+01, 1.25049203e-01,\n",
       "       3.76775519e+01, 5.71276937e+00, 1.25034970e-01, 1.25007413e-01,\n",
       "       1.25246429e-01, 1.68933849e+01, 3.93954338e+00, 2.32036533e+01,\n",
       "       9.02449698e+01, 1.27565154e+01, 1.25002381e-01, 1.26239773e+00,\n",
       "       1.25096764e-01, 1.25686190e+01, 6.18198777e+00, 1.66124695e+02,\n",
       "       1.33088692e+01, 1.25009187e-01, 1.25005572e-01, 1.76339621e+02,\n",
       "       1.48581860e+01, 1.25019587e-01, 1.67217648e+01, 1.38993807e+01,\n",
       "       5.15614873e+01, 1.47773257e+01, 4.14457492e+01, 3.45810831e+01,\n",
       "       1.54345124e+01, 2.42394333e+02, 8.56711948e+01, 5.57102784e+01,\n",
       "       1.25057905e-01, 8.93959101e+00, 3.43495666e+01, 3.44103087e+01,\n",
       "       7.25230269e+01, 3.63855706e+01, 1.25011085e-01, 2.56393956e+02,\n",
       "       8.62410242e+01, 1.69301184e-01, 1.25133058e-01, 8.35561626e+00,\n",
       "       4.70880571e+01, 8.07735253e+00, 1.38798767e+01, 2.23412288e+01,\n",
       "       3.87757313e+01, 1.04076206e+01, 3.48718297e+00, 1.25001624e-01,\n",
       "       2.56110009e+01, 3.50863077e+01, 2.68953674e+01, 6.40587656e+00,\n",
       "       1.25011735e-01, 1.25004432e-01, 1.25053754e-01, 7.08039683e+00,\n",
       "       4.61082456e-01, 1.44307392e+02, 6.04690619e+01, 9.16563355e+00,\n",
       "       1.25017569e-01, 1.25003750e-01, 1.13034170e+01, 9.90115714e+00,\n",
       "       1.46519507e+01, 2.90417103e+00, 1.25060555e-01, 1.26588815e+00,\n",
       "       4.48748506e+01, 4.39232379e+00, 1.54002671e+02, 2.99834438e+02,\n",
       "       1.45302997e+00, 1.25031574e-01, 1.25137766e-01, 1.08873392e+02,\n",
       "       7.87227998e+01, 7.34849870e+01, 3.09276147e+01, 1.87501768e+01,\n",
       "       1.98154487e-01, 5.21725110e-01, 1.25019715e-01, 1.90941305e+01,\n",
       "       1.28890016e-01, 1.76159772e+02, 1.87888991e+02, 2.35124803e+02,\n",
       "       4.10612271e+01, 3.32642020e+01, 2.81380783e+01, 1.25033282e-01,\n",
       "       6.61188220e+00, 5.50272442e-01, 3.79400622e+01, 2.59546790e+01,\n",
       "       3.74823288e+01, 1.27734711e+01, 4.32044340e+01, 2.00979743e+01,\n",
       "       1.41803814e+00, 1.25011524e-01, 1.99358468e+01, 2.11838110e+01,\n",
       "       2.70823806e+01, 1.67980700e+01, 5.72022107e+01, 5.94905950e+01,\n",
       "       3.49057678e+01, 6.11651595e+00, 1.25017415e-01, 1.05402070e+02,\n",
       "       5.65142775e+01, 4.79841432e+01, 2.52108867e+01, 2.06042788e+00,\n",
       "       2.60470364e+01, 4.51639536e+01, 1.16691149e+01, 5.08692714e+01,\n",
       "       1.25089705e-01, 3.78241338e+01, 9.99252449e+00, 3.75442416e+01,\n",
       "       1.56712206e+01, 1.77831052e+01, 1.25027731e-01, 9.63702557e+01,\n",
       "       2.24447415e+01, 3.19802140e+00, 2.94140067e+01, 1.82932661e+01,\n",
       "       2.51843978e+01, 1.81798177e+01, 1.25025455e-01, 5.29396596e+00,\n",
       "       4.57774633e+01, 1.29219696e+02, 6.14031830e+00, 4.89729938e+00,\n",
       "       2.72382147e+00, 5.10530205e+01, 6.36546747e+01, 4.66209122e+01,\n",
       "       1.52287573e+01, 1.95778594e+01, 2.03953028e+01, 3.10340317e+01,\n",
       "       4.83519806e+01, 1.55086312e+01, 4.55430115e+01, 2.11820611e+00,\n",
       "       1.67258204e+01, 1.38826615e+01, 6.06676733e+01, 2.37252040e+01,\n",
       "       1.25039334e-01, 1.25029859e-01, 1.48685382e+01, 2.46886903e+00,\n",
       "       6.17071739e+00, 6.84284532e+01, 1.04061438e+02, 4.22171599e+01,\n",
       "       5.29011282e+00, 7.48093712e+00, 1.42249729e+01, 7.60370537e+00,\n",
       "       1.84850845e+02, 1.54037745e+00, 1.04279794e+01, 2.63335321e+01,\n",
       "       2.61526243e+01, 5.19538350e+01, 6.00262483e+01, 6.14246852e+00,\n",
       "       1.25038285e-01, 1.25052813e-01, 4.15119344e+01, 4.33282305e-01,\n",
       "       1.13456369e+02, 5.03595387e+00, 2.03268827e+01, 3.84885438e+01,\n",
       "       1.90068236e+01, 1.65242778e+02, 4.93117134e+00, 2.20778378e+02,\n",
       "       1.25055795e-01, 8.74359752e+01, 7.20374424e+01, 1.68771442e+01,\n",
       "       2.90630388e+00, 4.11429983e+01, 9.97616728e+00, 3.57018728e+01,\n",
       "       6.25133677e+01, 1.52851594e+01, 2.01958415e+01, 1.25126709e-01,\n",
       "       1.25005297e-01, 6.55468449e-01, 1.85124738e+02, 1.15888957e+02,\n",
       "       2.40158461e+01, 2.10909549e+01, 3.67950384e+01, 1.33401276e+00,\n",
       "       6.70440315e+00, 5.74786162e+00, 1.25003295e-01, 1.25003582e-01,\n",
       "       1.11877214e+02, 1.25005506e-01, 6.48907519e+01, 9.09153450e+00,\n",
       "       5.13132766e+00, 1.08598706e+01, 1.25026483e-01, 1.25071345e-01,\n",
       "       1.25162511e-01, 5.55023264e+01, 1.42941721e+01, 1.26339721e-01,\n",
       "       3.16220010e+01, 7.97211572e+00, 1.47945523e+01, 1.25018166e-01,\n",
       "       5.68582532e+01, 3.01621937e+00, 9.82992812e+00, 7.67977782e+00,\n",
       "       1.44279916e+01, 4.81151308e+01, 1.11396984e+01, 4.75019951e+00,\n",
       "       1.25038084e-01, 9.62816256e-01, 1.25053748e-01, 4.09127016e+01,\n",
       "       1.07963541e+01, 5.38844720e+00, 1.32761443e+00, 2.64651969e+00,\n",
       "       1.68671919e+01, 1.04732644e+01, 1.25012404e-01, 1.49991953e-01,\n",
       "       1.25212139e-01, 1.18264022e+01, 2.89043923e+01, 2.33279158e+01,\n",
       "       1.25009695e-01, 6.20049954e+01, 2.20499421e+01, 2.12817653e+01,\n",
       "       5.86020450e+00, 3.53934547e+01, 5.74045575e+01, 6.17686910e+00,\n",
       "       1.17919074e+02, 2.62043885e+01, 3.06384810e+01, 1.14079035e+01,\n",
       "       4.60665666e+00, 3.00684200e+01, 1.25042665e-01, 3.80416243e+01,\n",
       "       8.56730385e+01, 1.86894135e+02, 6.00362748e+00, 4.28749965e+00,\n",
       "       2.04408040e+01, 9.70379740e+00, 1.25088236e-01, 2.79227503e+01,\n",
       "       5.66148577e+00, 3.65364731e+00, 3.22825149e+01, 7.92019238e+00,\n",
       "       1.86415619e+01, 2.05309411e+01, 8.35908779e+00, 3.59982855e+01,\n",
       "       7.95732201e+00, 4.21474194e+00, 1.25089136e-01, 1.25017959e-01,\n",
       "       3.46831420e+02, 2.00199834e+01, 1.15860934e+01, 3.00940126e+01,\n",
       "       2.05700143e+01, 8.13563435e+01, 4.03512520e+00, 1.29173537e+01,\n",
       "       1.15401655e+01, 1.25015713e-01, 8.43632337e+00, 2.23834204e+01,\n",
       "       9.74156684e+00, 1.23757312e+02, 1.25409558e-01, 1.22702756e+01,\n",
       "       1.25033331e-01, 2.09157447e+02, 8.49806653e+01, 8.25684295e+00,\n",
       "       4.69190964e+01, 2.38144277e+00, 5.50128554e+00, 1.25024685e-01,\n",
       "       1.25002642e-01, 1.25068818e-01, 1.30318815e+02, 1.25148915e-01,\n",
       "       1.57643017e+02, 2.66444796e+00, 1.25008033e-01, 6.81698036e+00,\n",
       "       2.44490936e+01, 1.25097092e-01, 1.99907054e+01, 2.07586374e+01,\n",
       "       1.25033484e-01, 1.25002535e-01, 1.25042427e-01, 4.67849856e+00,\n",
       "       1.45831405e+01, 2.18659920e+01, 2.63700301e+01, 1.78601404e+01,\n",
       "       1.25069172e-01, 1.63771048e+01, 8.56884631e+00, 4.03717593e+00,\n",
       "       6.52131951e+01, 6.30084058e+01, 1.07857673e+02, 1.25016120e-01,\n",
       "       2.53426197e+01, 2.27475163e+02, 1.22405471e+02, 1.45597277e+01,\n",
       "       1.25039970e-01, 1.25094736e-01, 2.55918400e+01, 1.19952554e+01,\n",
       "       6.73025640e+01, 1.25007868e-01, 4.30689685e+01, 2.57495300e+01,\n",
       "       1.17612405e+01, 1.20836007e+01, 5.71754237e+01, 4.57268861e+00,\n",
       "       1.25087718e-01, 1.25322195e-01, 5.95334731e+00, 1.51164390e+00,\n",
       "       1.25003956e-01, 1.25085150e-01, 1.86124865e+02, 1.42016872e+02,\n",
       "       1.82558942e+01, 2.96610925e-01, 6.77140112e-01, 1.25105840e-01,\n",
       "       5.78132196e+01, 9.01038402e+00, 1.68803317e+01, 6.82701438e+01,\n",
       "       1.01303497e+01, 6.74420604e+01, 3.53805719e+01, 3.97154948e+00,\n",
       "       4.27798535e+01, 1.28871442e+01, 4.23633793e+01, 1.25000307e-01,\n",
       "       1.25001119e-01, 4.83131484e+00, 6.89249316e+00, 2.03264847e+02,\n",
       "       1.25099581e-01, 1.25009296e-01, 1.25032441e-01, 1.25025365e-01,\n",
       "       6.38162588e+00, 4.81318727e+00, 8.76487397e+01, 1.01850668e+02,\n",
       "       1.25086988e-01, 4.30156613e+00, 9.24647456e+00, 4.97389733e+01,\n",
       "       5.20982344e+00, 5.06937059e+00, 6.29224078e+00, 5.93656722e+01,\n",
       "       2.51226499e+01, 2.59603548e+01, 1.16040523e+01, 1.63524619e+00,\n",
       "       5.70420177e+00, 1.55473689e+01, 1.26910435e-01, 1.25005610e-01,\n",
       "       1.25003808e-01, 1.25007965e-01, 1.25000914e-01, 1.25001785e-01,\n",
       "       1.25031199e-01, 1.25003530e-01, 7.03238993e+02, 3.06529434e+02,\n",
       "       4.04532346e+01, 3.02911688e+01, 8.66830093e+01, 6.79285199e+01])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Topic #0에 대해 피처 벡터화된 행렬\n",
    "lda.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e047e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0\n",
      "year 10 game medical health team 12 20 disease cancer 1993 games years patients good\n",
      "Topic # 1\n",
      "don just like know people said think time ve didn right going say ll way\n",
      "Topic # 2\n",
      "image file jpeg program gif images output format files color entry 00 use bit 03\n",
      "Topic # 3\n",
      "like know don think use does just good time book read information people used post\n",
      "Topic # 4\n",
      "armenian israel armenians jews turkish people israeli jewish government war dos dos turkey arab armenia 000\n",
      "Topic # 5\n",
      "edu com available graphics ftp data pub motif mail widget software mit information version sun\n",
      "Topic # 6\n",
      "god people jesus church believe christ does christian say think christians bible faith sin life\n",
      "Topic # 7\n",
      "use dos thanks windows using window does display help like problem server need know run\n"
     ]
    }
   ],
   "source": [
    "# 토픽별로 연관도가 높은 순으로 word를 나열하기 위한 함수 생성\n",
    "\n",
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_index, topic in enumerate(model.components_):\n",
    "        print('Topic #',topic_index)\n",
    "\n",
    "        # components_ array에서 가장 값이 큰 순으로 정렬했을 때, 그 값의 array index를 반환. \n",
    "        topic_word_indexes = topic.argsort()[::-1]\n",
    "        top_indexes=topic_word_indexes[:no_top_words]\n",
    "        \n",
    "        # top_indexes대상인 index별로 feature_names에 해당하는 word feature 추출 후 join으로 concat\n",
    "        feature_concat = ' '.join([feature_names[i] for i in top_indexes])                \n",
    "        print(feature_concat)\n",
    "\n",
    "# CountVectorizer객체내의 전체 word들의 명칭을 get_features_names( )를 통해 추출\n",
    "feature_names = count_vect.get_feature_names()\n",
    "\n",
    "# Topic별 가장 연관도가 높은 word를 15개만 추출\n",
    "display_topics(lda, feature_names, 15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
